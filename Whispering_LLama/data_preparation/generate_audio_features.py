import os
from dataclasses import dataclass, field, replace
from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union
import whisper_openAI.whisper as whisper
from whisper_openAI.whisper.tokenizer import Tokenizer, get_tokenizer
import torch
import torch.nn.functional as F
from torch import Tensor
import json
import sys
from pathlib import Path
import requests



from lit_llama.tokenizer import Tokenizer
from tqdm import tqdm

# We get the acoustic embeddings from Whisper Large V2
#model,processor = whisper.load_model("large-v2")
model,processor = whisper.load_model("medium")
#model = whisper.load_model("small")
model.eval()

with open(f'/home/user/Documents/NEU_SFU/speech_error_classification/Whispering-LLaMA/audio_features/tft_inferences.json', "r") as file: #change
    test_data = json.load(file)

#print(test_data)

"""Implementation derived from https://github.com/tloen/alpaca-lora"""

tokenizer_path: Path = Path("/home/user/Documents/NEU_SFU/speech_error_classification/Whispering-LLaMA/model/tokenizer.model")
tokenizer = Tokenizer(tokenizer_path)
train_set =test_data
print(f"train has {len(train_set):,} samples")
print("Processing train split ...")

result = []

def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:
    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)
 
instruction = 'You are an ASR transcript selector. You have a few transcripts generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors or word repetitions or mispelled words or mispronunciations, you will transcribe them to produce the most accurate and coherent transcript with the audio. I am expecting a faithful transcription rather than intended transcription' 

for i in tqdm(range(len(test_data))):        
    for name in test_data[i].keys():
        ip = test_data[i]
        
        
    inference = ip['inference']
    gt = ip['ground_truth']
        
    # Removing the ground_truth, if present among the inferences for the prompt
    if gt in inference:
        inference.remove(gt)
            
    # joining the inputs with '\n'
    for_input = '\n'.join(inference[:15])
    # The prompt follows the Aplaca template ; https://github.com/tatsu-lab/stanford_alpaca
    full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:\n{instruction}\n\n### Input:\n{for_input}\n\n### Response:"""
    full_prompt_and_response = full_prompt + gt

    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=2048, eos=False)
    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=2048)
    labels = encoded_full_prompt_and_response.clone()
    labels_with_masked_input = encoded_full_prompt_and_response.clone()
    labels_with_masked_input[:len(encoded_full_prompt)] = -1
    
    #path =  ip['path']
    path =  ip['original_full_path']
    audio = whisper.load_audio(path)  
    audio = whisper.pad_or_trim(audio)            
    mel = whisper.log_mel_spectrogram(audio).to(model.device)  # of shape (#channels =  80 ; #samples = )   
    mel = mel.unsqueeze(0)
    
    with torch.no_grad():
        audio_features  = model.encoder(mel)
        

    result.append( {**ip,'index':i, "input_ids": encoded_full_prompt_and_response, "input_ids_no_response": encoded_full_prompt, "labels": labels, 'labels_with_masked_input': labels_with_masked_input,'audio_features': audio_features.bfloat16()})


#print(result)

torch.save(result,'/home/user/Documents/NEU_SFU/speech_error_classification/Whispering-LLaMA/audio_features/ac_test.pt')







